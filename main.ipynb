{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "from models.ensemble import EnsembleModel, ModelWithHead, ReadoutAdapter\n",
    "\n",
    "try:\n",
    "    IterativeRefinementNet\n",
    "except NameError:\n",
    "    class IterativeRefinementNet(nn.Module):\n",
    "        def __init__(self, input_size, hidden_sizes, num_iterations, output_size, activation_str):\n",
    "            super().__init__()\n",
    "            act = {\"relu\": nn.ReLU(), \"tanh\": nn.Tanh(), \"sigmoid\": nn.Sigmoid(), \"gelu\": nn.GELU()}[activation_str]\n",
    "            layers = []\n",
    "            last = input_size\n",
    "            for h in hidden_sizes:\n",
    "                layers += [nn.Linear(last, h), act]\n",
    "                last = h\n",
    "            layers.append(nn.Linear(last, output_size))\n",
    "            self.net = nn.Sequential(*layers)\n",
    "        def forward(self, x):\n",
    "            return self.net(x.view(x.size(0), -1))\n",
    "\n",
    "try:\n",
    "    StandardFeedForwardNet\n",
    "except NameError:\n",
    "    class StandardFeedForwardNet(nn.Module):\n",
    "        def __init__(self, input_size, hidden_sizes, output_size, activation_str):\n",
    "            super().__init__()\n",
    "            act = {\"relu\": nn.ReLU(), \"tanh\": nn.Tanh(), \"sigmoid\": nn.Sigmoid(), \"gelu\": nn.GELU()}[activation_str]\n",
    "            layers = []\n",
    "            last = input_size\n",
    "            for h in hidden_sizes:\n",
    "                layers += [nn.Linear(last, h), act]\n",
    "                last = h\n",
    "            layers.append(nn.Linear(last, output_size))\n",
    "            self.net = nn.Sequential(*layers)\n",
    "        def forward(self, x):\n",
    "            return self.net(x.view(x.size(0), -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monk1_data(batch_size, data_root='./data'):\n",
    "    monk_dir = os.path.join(data_root, 'monk')\n",
    "    os.makedirs(monk_dir, exist_ok=True)\n",
    "    \n",
    "    train_file = os.path.join(monk_dir, 'monks-1.train')\n",
    "    test_file = os.path.join(monk_dir, 'monks-1.test')\n",
    "    \n",
    "    # Download if files don't exist\n",
    "    if not os.path.exists(train_file):\n",
    "        print(\"Downloading MONK-1 train data...\")\n",
    "        url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-1.train\"\n",
    "        r = requests.get(url)\n",
    "        with open(train_file, 'w') as f:\n",
    "            f.write(r.text)\n",
    "            \n",
    "    if not os.path.exists(test_file):\n",
    "        print(\"Downloading MONK-1 test data...\")\n",
    "        url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-1.test\"\n",
    "        r = requests.get(url)\n",
    "        with open(test_file, 'w') as f:\n",
    "            f.write(r.text)\n",
    "\n",
    "    # One-hot encoding definitions for 6 attributes (Total: 17 features)\n",
    "    attr_dims = [3, 3, 2, 3, 4, 2]\n",
    "    \n",
    "    def parse_monk_file(file_path):\n",
    "        features = []\n",
    "        labels = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if not parts:\n",
    "                    continue\n",
    "                \n",
    "                label = int(parts[0])\n",
    "                attrs = [int(a) for a in parts[1:-1]] # last part is ID\n",
    "                \n",
    "                # One-hot encode features\n",
    "                one_hot_features = []\n",
    "                for i, attr_val in enumerate(attrs):\n",
    "                    one_hot = torch.zeros(attr_dims[i])\n",
    "                    one_hot[attr_val - 1] = 1.0 # Values are 1-based\n",
    "                    one_hot_features.append(one_hot)\n",
    "                \n",
    "                features.append(torch.cat(one_hot_features))\n",
    "                labels.append(label)\n",
    "                \n",
    "        return torch.stack(features), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    print(\"Parsing MONK-1 data...\")\n",
    "    train_x, train_y = parse_monk_file(train_file)\n",
    "    test_x, test_y = parse_monk_file(test_file)\n",
    "    \n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "    test_dataset = TensorDataset(test_x, test_y)\n",
    "    \n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    input_size = 17\n",
    "    output_size = 2\n",
    "    \n",
    "    return train_loader, test_loader, input_size, output_size\n",
    "\n",
    "def get_ml_cup_data(batch_size, data_root='./data'):\n",
    "    # data will be in ./MLC25/\n",
    "    ml_cup_dir = os.path.join(data_root, 'MLC25')\n",
    "    os.makedirs(ml_cup_dir, exist_ok=True)\n",
    "    \n",
    "    train_file = os.path.join(ml_cup_dir, 'ML-CUP25-TR.csv')\n",
    "    test_file = os.path.join(ml_cup_dir, 'ML-CUP25-TS.csv')\n",
    "\n",
    "    if not os.path.exists(train_file):\n",
    "        print(\"Downloading ML-CUP25-TR train data...\")\n",
    "        url = \"https://gist.githubusercontent.com/FlavRomano/a19771d5c67f71dad557e5fa384db38b/raw/7290bff843b8a5c3a650457281c93c1d54e55f51/ML-CUP25-TR.csv\"\n",
    "        r = requests.get(url)\n",
    "        with open(train_file, 'w') as f:\n",
    "            f.write(r.text)\n",
    "            \n",
    "    if not os.path.exists(test_file):\n",
    "        print(\"Downloading ML-CUP25-TS test data...\")\n",
    "        url = \"https://gist.githubusercontent.com/FlavRomano/453dc2affc584028cb122d6b52cec295/raw/1cb1e84b26f8efd2ac081701d610c94498f988e1/ML-CUP25-TS.csv\"\n",
    "        r = requests.get(url)\n",
    "        with open(test_file, 'w') as f:\n",
    "            f.write(r.text)\n",
    "    \n",
    "    # --- Check if files exist ---\n",
    "    if not os.path.exists(train_file) or not os.path.exists(test_file):\n",
    "        print(\"---\" * 20)\n",
    "        print(f\"ERROR: ML-CUP dataset files not found.\")\n",
    "        print(f\"This script cannot download the ML-CUP dataset automatically.\")\n",
    "        print(f\"Please manually place your dataset files at these locations:\")\n",
    "        print(f\"Training data: {os.path.abspath(train_file)}\")\n",
    "        print(f\"Test data:     {os.path.abspath(test_file)}\")\n",
    "        print(\"---\" * 20)\n",
    "        sys.exit(1) # Stop the script\n",
    "\n",
    "    # --- Parser for ML-CUP data ---\n",
    "    # This parser assumes the standard ML-CUP format:\n",
    "    # - Lines starting with '#' are comments\n",
    "    # - Data is comma-separated\n",
    "    # - Column 0: ID (ignored)\n",
    "    # - Columns 1-10: 10 input features\n",
    "    # - Columns 11-12: 2 output targets (regression)\n",
    "    def parse_ml_cup_file(file_path):\n",
    "        features = []\n",
    "        labels = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line or line.startswith('#'):\n",
    "                    continue\n",
    "                \n",
    "                parts = line.split(',')\n",
    "                if len(parts) < 13:\n",
    "                    print(f\"Warning: Skipping malformed line: {line}\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Features are columns 1 through 10 (10 features)\n",
    "                    feature_values = [float(p) for p in parts[1:11]]\n",
    "                    # Labels are columns 11 and 12 (2 targets)\n",
    "                    label_values = [float(p) for p in parts[11:13]]\n",
    "                    \n",
    "                    features.append(torch.tensor(feature_values, dtype=torch.float32))\n",
    "                    labels.append(torch.tensor(label_values, dtype=torch.float32))\n",
    "                except ValueError as e:\n",
    "                    print(f\"Warning: Skipping line due to parsing error ({e}): {line}\")\n",
    "\n",
    "        return torch.stack(features), torch.stack(labels)\n",
    "\n",
    "    print(\"Parsing ML-CUP data...\")\n",
    "    train_x, train_y = parse_ml_cup_file(train_file)\n",
    "    test_x, test_y = parse_ml_cup_file(test_file)\n",
    "    \n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "    test_dataset = TensorDataset(test_x, test_y)\n",
    "    \n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Based on the parser above\n",
    "    input_size = 10\n",
    "    output_size = 2\n",
    "    \n",
    "    print(\"---\" * 10)\n",
    "    print(\"WARNING: The 'step_out' model uses a final-state activation (e.g., ReLU).\")\n",
    "    print(\"This is likely unsuitable for the ML-CUP regression task (which can have\")\n",
    "    print(\"negative targets). The 'standard' model is recommended.\")\n",
    "    print(\"---\" * 10)\n",
    "    \n",
    "    return train_loader, test_loader, input_size, output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "## configuration presets\n",
    "with open('./config/presets.json') as presets_fd:\n",
    "    config_presets = json.load(presets_fd)\n",
    "    print(\"presets loaded\")\n",
    "\n",
    "def select_preset(key: str):\n",
    "    \"\"\"Flattens ensemble presets into a compact cfg the rest\n",
    "    of the notebook can consume easily.\"\"\"\n",
    "    base = config_presets[key]\n",
    "    if base.get(\"model\") != \"ensemble\":\n",
    "        return base  # single-model cfg as-is\n",
    "\n",
    "    flat = {\n",
    "        \"model\": \"ensemble\",\n",
    "        # shared training knobs:\n",
    "        \"dataset\": base[\"shared\"][\"dataset\"],\n",
    "        \"epochs\": base[\"shared\"][\"epochs\"],\n",
    "        \"batch_size\": base[\"shared\"][\"batch_size\"],\n",
    "        # ensemble-specific:\n",
    "        \"weights\": base.get(\"weights\", [1.0] * len(base[\"members\"])),\n",
    "        \"members\": base[\"members\"]\n",
    "    }\n",
    "    return flat\n",
    "\n",
    "cfg = select_preset(\"custom_ensemble\") # <- change this with the choosen preset\n",
    "\n",
    "if cfg[\"model\"] == \"ensemble\":\n",
    "    # Use first member as representative for globals\n",
    "    first_member = cfg[\"members\"][0]\n",
    "    HIDDEN_SIZES = first_member[\"hidden_sizes\"]\n",
    "    ACTIVATION   = first_member[\"activation\"]\n",
    "    LEARNING_RATE = first_member.get(\"lr\", 0.001)\n",
    "else:\n",
    "    HIDDEN_SIZES = cfg[\"hidden_sizes\"]\n",
    "    ACTIVATION   = cfg[\"activation\"]\n",
    "    LEARNING_RATE = cfg.get(\"lr\", 0.001)\n",
    "\n",
    "BATCH_SIZE = cfg[\"batch_size\"]\n",
    "NUM_EPOCHS = cfg[\"epochs\"]\n",
    "\n",
    "data_root = \"./data\"\n",
    "is_regression_task = False\n",
    "\n",
    "print(\"Config:\", json.dumps(cfg, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loading + task type\n",
    "\n",
    "train_loader = None\n",
    "test_loader = None\n",
    "INPUT_SIZE = None\n",
    "OUTPUT_SIZE = None\n",
    "metric_name = \"Test Accuracy\"\n",
    "\n",
    "print(f\"Loading dataset: {cfg['dataset'].upper()}\")\n",
    "\n",
    "if cfg[\"dataset\"] == \"monk1\":\n",
    "    train_loader, test_loader, INPUT_SIZE, OUTPUT_SIZE = get_monk1_data(BATCH_SIZE, data_root)\n",
    "    metric_name = \"Test monk\"\n",
    "\n",
    "elif cfg[\"dataset\"] == \"mlc25\":\n",
    "    train_loader, test_loader, INPUT_SIZE, OUTPUT_SIZE = get_ml_cup_data(BATCH_SIZE)\n",
    "    is_regression_task = True\n",
    "    metric_name = \"Test MSE\"\n",
    "\n",
    "else:\n",
    "    if cfg[\"dataset\"] == \"mnist\":\n",
    "        normalize_mean, normalize_std = (0.1307,), (0.3081,)\n",
    "        dataset_class = torchvision.datasets.MNIST\n",
    "    elif cfg[\"dataset\"] == \"fmnist\":\n",
    "        normalize_mean, normalize_std = (0.2860,), (0.3530,)\n",
    "        dataset_class = torchvision.datasets.FashionMNIST\n",
    "    elif cfg[\"dataset\"] == \"kmnist\":\n",
    "        normalize_mean, normalize_std = (0.1918,), (0.3483,)\n",
    "        dataset_class = torchvision.datasets.KMNIST\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {cfg['dataset']}\")\n",
    "\n",
    "    INPUT_SIZE = 28 * 28\n",
    "    OUTPUT_SIZE = 10\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(normalize_mean, normalize_std)\n",
    "    ])\n",
    "\n",
    "    train_dataset = dataset_class(root=data_root, train=True, transform=transform, download=True)\n",
    "    test_dataset  = dataset_class(root=data_root, train=False, transform=transform, download=True)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(dataset=test_dataset,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Detected Input Size: {INPUT_SIZE}, Output Size: {OUTPUT_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps'); print(\"Using device: Apple MPS\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda'); print(\"Using device: NVIDIA CUDA\")\n",
    "else:\n",
    "    device = torch.device('cpu'); print(\"Using device: CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_member_model(member_cfg, input_size, output_size, device):\n",
    "    name = member_cfg[\"name\"].lower()\n",
    "    act  = member_cfg[\"activation\"]\n",
    "    hids = member_cfg[\"hidden_sizes\"]\n",
    "\n",
    "    if name == \"standard\":\n",
    "        base = StandardFeedForwardNet(\n",
    "            input_size=input_size,\n",
    "            hidden_sizes=hids,\n",
    "            output_size=output_size,\n",
    "            activation_str=act\n",
    "        ).to(device)\n",
    "    elif name == \"step_out\":\n",
    "        base = IterativeRefinementNet(\n",
    "            input_size=input_size,\n",
    "            hidden_sizes=hids,\n",
    "            num_iterations=len(hids),\n",
    "            output_size=output_size,\n",
    "            activation_str=act\n",
    "        ).to(device)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown ensemble member name: {name}\")\n",
    "    return base\n",
    "\n",
    "# --- Build models from cfg (single or ensemble) ---\n",
    "members_wrapped = []\n",
    "optimizers = []\n",
    "ensemble = None\n",
    "\n",
    "# --- Task mode from your dataset branch ---\n",
    "task_mode = \"regression\" if is_regression_task else \"classification\"\n",
    "\n",
    "if cfg.get(\"model\") == \"ensemble\":\n",
    "    # Expect: cfg[\"members\"] (list of dicts) and cfg[\"weights\"]\n",
    "    bases = [build_member_model(m, INPUT_SIZE, OUTPUT_SIZE, device) for m in cfg[\"members\"]]\n",
    "    # add readout heads (classification needs logits)\n",
    "    adapters = [ReadoutAdapter(OUTPUT_SIZE, OUTPUT_SIZE, task_mode) for _ in bases]\n",
    "    members_wrapped = [ModelWithHead(b, a).to(device) for b, a in zip(bases, adapters)]\n",
    "    # per-member lrs (from preset); fallback to global LEARNING_RATE if missing\n",
    "    for wrapped, m in zip(members_wrapped, cfg[\"members\"]):\n",
    "        lr_m = m.get(\"lr\", LEARNING_RATE)\n",
    "        optimizers.append(optim.Adam(wrapped.parameters(), lr=lr_m))\n",
    "    # ensemble with weights\n",
    "    ensemble = EnsembleModel(members_wrapped, weights=cfg.get(\"weights\", None)).to(device)\n",
    "\n",
    "    print(f\"Ensemble with {len(members_wrapped)} members. Weights:\", \n",
    "          (ensemble.weights / ensemble.weights.sum()).cpu().tolist())\n",
    "    for i, (mw, opt) in enumerate(zip(members_wrapped, optimizers), 1):\n",
    "        params = sum(p.numel() for p in mw.parameters())\n",
    "        print(f\" Member {i}: params={params}, lr={opt.param_groups[0]['lr']}\")\n",
    "\n",
    "else:\n",
    "    # Single-model path (keeps your previous behavior)\n",
    "    if cfg[\"model\"] == \"standard\":\n",
    "        base = StandardFeedForwardNet(INPUT_SIZE, cfg[\"hidden_sizes\"], OUTPUT_SIZE, cfg[\"activation\"]).to(device)\n",
    "    elif cfg[\"model\"] == \"step_out\":\n",
    "        base = IterativeRefinementNet(INPUT_SIZE, cfg[\"hidden_sizes\"], len(cfg[\"hidden_sizes\"]), OUTPUT_SIZE, cfg[\"activation\"]).to(device)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {cfg['model']}\")\n",
    "    # classification: ensure logits; regression: identity\n",
    "    wrapped = ModelWithHead(base, ReadoutAdapter(OUTPUT_SIZE, OUTPUT_SIZE, task_mode)).to(device)\n",
    "    members_wrapped = [wrapped]\n",
    "    optimizers = [optim.Adam(wrapped.parameters(), lr=LEARNING_RATE)]\n",
    "    ensemble = None  # not used\n",
    "    print(\"Single model params:\", sum(p.numel() for p in wrapped.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criterion, optimizers, trackers (single or ensemble)\n",
    "\n",
    "criterion = nn.MSELoss() if is_regression_task else nn.CrossEntropyLoss()\n",
    "\n",
    "# Build training targets depending on cfg[\"model\"]\n",
    "# - Single: `model` already exists, we wrap it into lists for uniform handling\n",
    "# - Ensemble: expect `members_wrapped`, `optimizers` already created (from Cell A/B)\n",
    "if cfg.get(\"model\") == \"ensemble\":\n",
    "    train_members = members_wrapped                 # list[nn.Module]\n",
    "    member_opts   = optimizers                      # list[torch.optim.Optimizer]\n",
    "    ensemble_model = ensemble                       # EnsembleModel (or None)\n",
    "else:\n",
    "    # single-model fallback (use your existing 'model' and 'optimizer')\n",
    "    train_members = [base]\n",
    "    member_opts   = [optim.Adam(base.parameters(), lr=LEARNING_RATE)]\n",
    "    ensemble_model = None\n",
    "\n",
    "epoch_train_losses = []\n",
    "epoch_test_individual = []   # per-member metric each epoch\n",
    "epoch_test_ensemble = []     # ensemble metric each epoch (if any)\n",
    "\n",
    "print(f\"Training for {NUM_EPOCHS} epochs on \"\n",
    "      f\"{'regression' if is_regression_task else 'classification'} task \"\n",
    "      f\"with {'ensemble' if cfg.get('model')=='ensemble' else 'single model'}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop (single or ensemble) + evaluation\n",
    "\n",
    "def evaluate_all(members, ensemble_model, loader, device, is_regression, criterion):\n",
    "    for m in members:\n",
    "        m.eval()\n",
    "    if ensemble_model is not None:\n",
    "        ensemble_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if is_regression:\n",
    "            mses = [0.0 for _ in members]\n",
    "            mse_ens = 0.0\n",
    "            n = 0\n",
    "            for data, labels in loader:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                preds = [m(data) for m in members]\n",
    "                for i, p in enumerate(preds):\n",
    "                    mses[i] += criterion(p, labels).item()\n",
    "                if ensemble_model is not None:\n",
    "                    mse_ens += criterion(ensemble_model(data), labels).item()\n",
    "                n += 1\n",
    "            mses = [m / n for m in mses]\n",
    "            mse_ens = (mse_ens / n) if ensemble_model is not None else None\n",
    "            return {\"individual\": mses, \"ensemble\": mse_ens}\n",
    "        else:\n",
    "            corrects = [0 for _ in members]\n",
    "            correct_ens = 0\n",
    "            total = 0\n",
    "            for data, labels in loader:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                logits_list = [m(data) for m in members]\n",
    "                for i, logits in enumerate(logits_list):\n",
    "                    _, pred = torch.max(logits, 1)\n",
    "                    corrects[i] += (pred == labels).sum().item()\n",
    "                if ensemble_model is not None:\n",
    "                    logits_ens = ensemble_model(data)\n",
    "                    _, pred_ens = torch.max(logits_ens, 1)\n",
    "                    correct_ens += (pred_ens == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "            accs = [100.0 * c / total for c in corrects]\n",
    "            acc_ens = (100.0 * correct_ens / total) if ensemble_model is not None else None\n",
    "            return {\"individual\": accs, \"ensemble\": acc_ens}\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for m in train_members:\n",
    "        m.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        # forward per member\n",
    "        outs = [m(data) for m in train_members]\n",
    "        # per-member losses (same criterion for both tasks)\n",
    "        losses = [criterion(o, labels) for o in outs]\n",
    "        total_loss = sum(losses)  # equal weights; tweak if desired\n",
    "\n",
    "        # optimize\n",
    "        for opt in member_opts:\n",
    "            opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        for opt in member_opts:\n",
    "            opt.step()\n",
    "\n",
    "        running_loss += total_loss.item()\n",
    "\n",
    "        # logging\n",
    "        log_frequency = max(1, len(train_loader) // 4)\n",
    "        if (i + 1) % log_frequency == 0 or (i + 1) == len(train_loader):\n",
    "            loss_str = \"  \".join([f\"L{i+1}:{l.item():.4f}\" for i, l in enumerate(losses)])\n",
    "            print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Step [{i+1}/{len(train_loader)}], {loss_str}  Sum:{total_loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    epoch_train_losses.append(avg_train_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] complete. Average Training Loss (sum of members): {avg_train_loss:.4f}\")\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    metrics = evaluate_all(train_members, ensemble_model, test_loader, device, is_regression_task, criterion)\n",
    "    epoch_test_individual.append(metrics[\"individual\"])\n",
    "    if metrics[\"ensemble\"] is not None:\n",
    "        epoch_test_ensemble.append(metrics[\"ensemble\"])\n",
    "\n",
    "    if is_regression_task:\n",
    "        if metrics[\"ensemble\"] is None:\n",
    "            print(\"Average Test MSEs -> \" + \"  \".join([f\"M{i+1}:{m:.4f}\" for i, m in enumerate(metrics['individual'])]))\n",
    "        else:\n",
    "            print(\"Average Test MSEs -> \" + \"  \".join([f\"M{i+1}:{m:.4f}\" for i, m in enumerate(metrics['individual'])]) +\n",
    "                  f\", Ensemble:{metrics['ensemble']:.4f} (lower is better)\")\n",
    "    else:\n",
    "        if metrics[\"ensemble\"] is None:\n",
    "            print(\"Accuracies -> \" + \"  \".join([f\"M{i+1}:{m:.2f}%\" for i, m in enumerate(metrics['individual'])]))\n",
    "        else:\n",
    "            print(\"Accuracies -> \" + \"  \".join([f\"M{i+1}:{m:.2f}%\" for i, m in enumerate(metrics['individual'])]) +\n",
    "                  f\", Ensemble:{metrics['ensemble']:.2f}%\")\n",
    "\n",
    "print(\"Finished Training!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating plot...\")\n",
    "epochs_range = range(1, NUM_EPOCHS + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# ---- Left subplot: Training loss ----\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, epoch_train_losses, 'o-', label='Training Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# ---- Right subplot: Test metrics ----\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "if cfg.get(\"model\") == \"ensemble\":\n",
    "    # Each element in epoch_test_individual = list of member metrics\n",
    "    n_members = len(epoch_test_individual[0])\n",
    "    for m_idx in range(n_members):\n",
    "        vals = [ep[m_idx] for ep in epoch_test_individual]\n",
    "        label = f\"Member {m_idx+1}\"\n",
    "        plt.plot(epochs_range, vals, 'o-', label=label)\n",
    "\n",
    "    # Ensemble curve if available\n",
    "    if len(epoch_test_ensemble) > 0:\n",
    "        plt.plot(epochs_range, epoch_test_ensemble, 'o-', linewidth=2.5, label=\"Ensemble\", alpha=0.8)\n",
    "        ylabel = \"MSE\" if is_regression_task else \"Accuracy (%)\"\n",
    "    else:\n",
    "        ylabel = \"Metric\"\n",
    "\n",
    "else:\n",
    "    # Single-model fallback\n",
    "    plt.plot(epochs_range, epoch_test_individual, 'o-', label='Test Metric')\n",
    "    ylabel = metric_name\n",
    "\n",
    "plt.title('Test Metrics')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(ylabel)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.suptitle(f\"Training Metrics ({cfg['dataset'].upper()} - {cfg['model'].title()})\")\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
